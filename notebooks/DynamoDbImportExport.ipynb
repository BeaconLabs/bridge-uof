{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DynamoDB Import/Export Notebook\n",
    "\n",
    "Author: everett@bayesimpact.org\n",
    "\n",
    "Temporary solution to our need to back up (and possibly restore) dynamodb tables. Change the settings in the next cell, and then run the entire notebook.\n",
    "\n",
    "Good documentation on boto3, the python library for AWS APIs.\n",
    "* [boto3.resource](http://boto3.readthedocs.io/en/latest/guide/dynamodb.html) (which you should use whenever possible)\n",
    "* [boto3.client](http://boto3.readthedocs.io/en/latest/reference/services/dynamodb.html) (a more complete, but lower level API)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Configure this block, then run the whole notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set either RUN_IMPORT or RUN_EXPORT to true.\n",
    "# Here we set both for the sake of example to see\n",
    "# them both in action.\n",
    "RUN_EXPORT = False\n",
    "EXPORT_FILENAME = \"dynamodb.pickle\"\n",
    "\n",
    "RUN_IMPORT = False\n",
    "IMPORT_FILENAME = \"dynamodb.pickle\"\n",
    "IMPORT_TABLES_ADD_PREFIX = \"\"\n",
    "\n",
    "# AWS keys and DynamoDB region\n",
    "# If these are left blank, we'll try to find environment\n",
    "# variables by the same name and use those.\n",
    "AWS_ACCESS_KEY_ID = ''\n",
    "AWS_SECRET_ACCESS_KEY = ''\n",
    "AWS_REGION = ''\n",
    "\n",
    "# If this is true, we connect to a dynamodb-local instance\n",
    "# running on port 8000. The AWS keys above are then\n",
    "# irrelevant, unless you changed it to not run as a --sharedDb.\n",
    "USE_DYNAMODB_LOCAL = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution plan:\n",
      "* Exporting data from local dynamodb to dynamodb.pickle\n",
      "* Importing data from dynamodb.pickle to local dynamodb\n"
     ]
    }
   ],
   "source": [
    "print(\"Execution plan:\")\n",
    "db_type = \"local\" if USE_DYNAMODB_LOCAL else \"remote (%s)\" % AWS_REGION\n",
    "if RUN_EXPORT:\n",
    "    print(\"* Exporting data from %s dynamodb to %s\" % (db_type, EXPORT_FILENAME))\n",
    "if RUN_IMPORT:\n",
    "    print(\"* Importing data from %s to %s dynamodb\" % (IMPORT_FILENAME, db_type))\n",
    "if not (RUN_IMPORT or RUN_EXPORT):\n",
    "    raise ValueError(\"Nothing to do! Please set RUN_EXPORT or RUN_IMPORT to True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import copy\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "global client, resource\n",
    "\n",
    "def set_local_db():\n",
    "    global client, resource\n",
    "    # We are running in a container. The local dynamodb is attached\n",
    "    # to port 8000 of the host, so we need to connect to the host IP\n",
    "    # as seen by the container.\n",
    "    docker_host_ip = !ip route show | awk '/default/ {print $3}'\n",
    "    docker_host_ip = docker_host_ip[0]\n",
    "    args = {\n",
    "        'endpoint_url': 'http://' + docker_host_ip + ':8000',\n",
    "        'aws_access_key_id': 'foo',\n",
    "        'aws_secret_access_key': 'bar',\n",
    "        'region_name': 'baz'\n",
    "    }\n",
    "    client = boto3.client('dynamodb', **args)\n",
    "    resource = boto3.resource('dynamodb', **args)\n",
    "\n",
    "def set_client(access=AWS_ACCESS_KEY_ID, secret=AWS_SECRET_ACCESS_KEY, region=AWS_REGION):\n",
    "    global client, resource\n",
    "    args = {\n",
    "        'aws_access_key_id': access or os.environ['AWS_ACCESS_KEY_ID'],\n",
    "        'aws_secret_access_key': secret or os.environ['AWS_SECRET_ACCESS_KEY'],\n",
    "        'region_name': region or os.environ['AWS_REGION']\n",
    "    }\n",
    "    client = boto3.client('dynamodb', **args)\n",
    "    resource = boto3.resource('dynamodb', **args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scan_all_items(table_name, index_name=None):\n",
    "    items = []\n",
    "    start_key = None\n",
    "    table = resource.Table(table_name)\n",
    "    while True:\n",
    "        args = {}\n",
    "        if index_name:\n",
    "            args['IndexName'] = index_name\n",
    "        if start_key:\n",
    "            args['ExclusiveStartKey'] = start_key\n",
    "        response = table.scan(**args)\n",
    "        items.extend(response['Items'])\n",
    "        start_key = response.get('LastEvaluatedKey', None)\n",
    "        if start_key is None:\n",
    "            break\n",
    "        print(\"Paginated, %d items so far\" % len(items))\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_up_table_schema(schema, rename_prefix=None):\n",
    "    schema = copy.deepcopy(schema)\n",
    "    index_names = ['GlobalSecondaryIndexes', 'LocalSecondaryIndexes']\n",
    "    table_keys = ['AttributeDefinitions', 'KeySchema', 'ProvisionedThroughput',\n",
    "                 'TableName'] + index_names\n",
    "    index_keys = ['IndexName', 'KeySchema', 'Projection', 'ProvisionedThroughput']\n",
    "    if rename_prefix:\n",
    "        schema['TableName'] = rename_prefix + schema['TableName']\n",
    "        for name in index_names:\n",
    "            for index in schema.get(name, []):\n",
    "                index['IndexName'] = rename_prefix + index['IndexName']\n",
    "    \n",
    "    # Clean up indices\n",
    "    for name in index_names:\n",
    "        for index in schema.get(name, []):\n",
    "            keys = list(index.keys())\n",
    "            for k in keys:\n",
    "                if k not in index_keys:\n",
    "                    del index[k]\n",
    "            index['ProvisionedThroughput'] = {\n",
    "                'ReadCapacityUnits': index['ProvisionedThroughput']['ReadCapacityUnits'],\n",
    "                'WriteCapacityUnits': index['ProvisionedThroughput']['WriteCapacityUnits'],\n",
    "            }\n",
    "\n",
    "    # Clean up table\n",
    "    keys = list(schema.keys())\n",
    "    for k in keys:\n",
    "        if k not in table_keys:\n",
    "            del schema[k]\n",
    "    schema['ProvisionedThroughput'] = {\n",
    "        'ReadCapacityUnits': schema['ProvisionedThroughput']['ReadCapacityUnits'],\n",
    "        'WriteCapacityUnits': schema['ProvisionedThroughput']['WriteCapacityUnits'],\n",
    "    }\n",
    "    return schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_table_data(name):\n",
    "    result = {}\n",
    "    result['Items'] = scan_all_items(name)\n",
    "    result['Table'] = client.describe_table(TableName=name)['Table']\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_table_data(table_prefix=''):\n",
    "    data = {}\n",
    "    table_names = [t.name for t in resource.tables.all() if t.name.startswith(table_prefix)]\n",
    "    for name in table_names:\n",
    "        data[name] = get_table_data(name)\n",
    "        describe_table_data(data[name])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def describe_table_data(table_data):\n",
    "    print(\"%40s:%8d items %8.0f kB\" % (\n",
    "            table_data['Table']['TableName'],\n",
    "            len(table_data['Items']),\n",
    "            table_data['Table']['TableSizeBytes'] / 1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 0: Establish a connection and make sure it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found table ursus_auditentries\n",
      "Found table ursus_changedfields\n",
      "Found table ursus_events\n",
      "Found table ursus_feedbacks\n",
      "Found table ursus_general_infos\n",
      "Found table ursus_globalstates\n",
      "Found table ursus_incidents\n",
      "Found table ursus_involved_civilians\n",
      "Found table ursus_involved_officers\n",
      "Found table ursus_screeners\n",
      "Found table ursus_users\n",
      "Found table ursus_visits\n"
     ]
    }
   ],
   "source": [
    "set_local_db() if USE_DYNAMODB_LOCAL else set_client()\n",
    "for t in resource.tables.all():\n",
    "    print(\"Found table\", t.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Export an entire database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      ursus_auditentries:       0 items        0 kB\n",
      "                     ursus_changedfields:       0 items        0 kB\n",
      "                            ursus_events:       2 items        0 kB\n",
      "                         ursus_feedbacks:       0 items        0 kB\n",
      "                     ursus_general_infos:       0 items        0 kB\n",
      "                      ursus_globalstates:       1 items        0 kB\n",
      "                         ursus_incidents:       0 items        0 kB\n",
      "                ursus_involved_civilians:       0 items        0 kB\n",
      "                 ursus_involved_officers:       0 items        0 kB\n",
      "                         ursus_screeners:       0 items        0 kB\n",
      "                             ursus_users:       1 items        0 kB\n",
      "                            ursus_visits:       1 items        0 kB\n",
      "Saved 9692 bytes to dynamodb.pickle\n"
     ]
    }
   ],
   "source": [
    "if RUN_EXPORT:\n",
    "    data = get_all_table_data(table_prefix='')\n",
    "    with open('dynamodb.pickle' , \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(\"Saved %d bytes to %s\" % (os.stat(EXPORT_FILENAME).st_size, EXPORT_FILENAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - import from pickled export (task 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 9692 bytes of pickled data from dynamodb.pickle\n",
      "Loaded data for 12 tables\n",
      "                            ursus_events:       2 items        0 kB\n",
      "                      ursus_auditentries:       0 items        0 kB\n",
      "                 ursus_involved_officers:       0 items        0 kB\n",
      "                ursus_involved_civilians:       0 items        0 kB\n",
      "                         ursus_feedbacks:       0 items        0 kB\n",
      "                     ursus_changedfields:       0 items        0 kB\n",
      "                             ursus_users:       1 items        0 kB\n",
      "                         ursus_screeners:       0 items        0 kB\n",
      "                     ursus_general_infos:       0 items        0 kB\n",
      "                            ursus_visits:       1 items        0 kB\n",
      "                         ursus_incidents:       0 items        0 kB\n",
      "                      ursus_globalstates:       1 items        0 kB\n",
      "Importing...\n",
      "** Importing into table ursus_events **\n",
      "ursus_events already exists, deleting it first and starting afresh.\n",
      "Delete operation sent. Waiting until table no longer exists.\n",
      "Create operation sent. Waiting until table exists.\n",
      "Populating ursus_events with 2 items\n",
      "Done.\n",
      "** Importing into table ursus_auditentries **\n",
      "ursus_auditentries already exists, deleting it first and starting afresh.\n",
      "Delete operation sent. Waiting until table no longer exists.\n",
      "Create operation sent. Waiting until table exists.\n",
      "Populating ursus_auditentries with 0 items\n",
      "Done.\n",
      "** Importing into table ursus_involved_officers **\n",
      "ursus_involved_officers already exists, deleting it first and starting afresh.\n",
      "Delete operation sent. Waiting until table no longer exists.\n",
      "Create operation sent. Waiting until table exists.\n",
      "Populating ursus_involved_officers with 0 items\n",
      "Done.\n",
      "** Importing into table ursus_involved_civilians **\n",
      "ursus_involved_civilians already exists, deleting it first and starting afresh.\n",
      "Delete operation sent. Waiting until table no longer exists.\n",
      "Create operation sent. Waiting until table exists.\n",
      "Populating ursus_involved_civilians with 0 items\n",
      "Done.\n",
      "** Importing into table ursus_feedbacks **\n",
      "ursus_feedbacks already exists, deleting it first and starting afresh.\n",
      "Delete operation sent. Waiting until table no longer exists.\n",
      "Create operation sent. Waiting until table exists.\n",
      "Populating ursus_feedbacks with 0 items\n",
      "Done.\n",
      "** Importing into table ursus_changedfields **\n",
      "ursus_changedfields already exists, deleting it first and starting afresh.\n",
      "Delete operation sent. Waiting until table no longer exists.\n",
      "Create operation sent. Waiting until table exists.\n",
      "Populating ursus_changedfields with 0 items\n",
      "Done.\n",
      "** Importing into table ursus_users **\n",
      "ursus_users already exists, deleting it first and starting afresh.\n",
      "Delete operation sent. Waiting until table no longer exists.\n",
      "Create operation sent. Waiting until table exists.\n",
      "ACTIVE index: ori\n",
      "Populating ursus_users with 1 items\n",
      "Done.\n",
      "** Importing into table ursus_screeners **\n",
      "ursus_screeners already exists, deleting it first and starting afresh.\n",
      "Delete operation sent. Waiting until table no longer exists.\n",
      "Create operation sent. Waiting until table exists.\n",
      "Populating ursus_screeners with 0 items\n",
      "Done.\n",
      "** Importing into table ursus_general_infos **\n",
      "ursus_general_infos already exists, deleting it first and starting afresh.\n",
      "Delete operation sent. Waiting until table no longer exists.\n",
      "Create operation sent. Waiting until table exists.\n",
      "Populating ursus_general_infos with 0 items\n",
      "Done.\n",
      "** Importing into table ursus_visits **\n",
      "ursus_visits already exists, deleting it first and starting afresh.\n",
      "Delete operation sent. Waiting until table no longer exists.\n",
      "Create operation sent. Waiting until table exists.\n",
      "Populating ursus_visits with 1 items\n",
      "Done.\n",
      "** Importing into table ursus_incidents **\n",
      "ursus_incidents already exists, deleting it first and starting afresh.\n",
      "Delete operation sent. Waiting until table no longer exists.\n",
      "Create operation sent. Waiting until table exists.\n",
      "ACTIVE index: incident_id_str\n",
      "Populating ursus_incidents with 0 items\n",
      "Done.\n",
      "** Importing into table ursus_globalstates **\n",
      "ursus_globalstates already exists, deleting it first and starting afresh.\n",
      "Delete operation sent. Waiting until table no longer exists.\n",
      "Create operation sent. Waiting until table exists.\n",
      "Populating ursus_globalstates with 1 items\n",
      "Done.\n",
      "---- Data import complete\n"
     ]
    }
   ],
   "source": [
    "if RUN_IMPORT:\n",
    "    print(\"Loading %d bytes of pickled data from %s\" % (\n",
    "          os.stat(IMPORT_FILENAME).st_size, IMPORT_FILENAME))\n",
    "    data = None\n",
    "    with open(IMPORT_FILENAME, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    print(\"Loaded data for %d tables\" % len(data))\n",
    "    for table_name, table_data in data.items():\n",
    "        describe_table_data(table_data)\n",
    "\n",
    "    print(\"Importing...\")\n",
    "    existing_table_names = set([t.name for t in resource.tables.all()])\n",
    "    for table_name, info in data.items():\n",
    "        new_name = IMPORT_TABLES_ADD_PREFIX + table_name\n",
    "        print(\"** Importing into table %s **\" % new_name)\n",
    "\n",
    "        # Delete table if it already exists\n",
    "        if new_name in existing_table_names:\n",
    "            print(new_name, \"already exists, deleting it first and starting afresh.\")\n",
    "            client.delete_table(TableName=new_name)\n",
    "            print(\"Delete operation sent. Waiting until table no longer exists.\")\n",
    "            resource.Table(new_name).wait_until_not_exists()\n",
    "\n",
    "        # Create a new table\n",
    "        schema = clean_up_table_schema(info[\"Table\"], rename_prefix=IMPORT_TABLES_ADD_PREFIX)\n",
    "        items = info[\"Items\"]\n",
    "        client.create_table(**schema)\n",
    "        print(\"Create operation sent. Waiting until table exists.\")\n",
    "        tbl = resource.Table(new_name)\n",
    "        tbl.wait_until_exists()\n",
    "        if tbl.global_secondary_indexes:\n",
    "            for x in tbl.global_secondary_indexes:\n",
    "                print(x[\"IndexStatus\"], \"index:\", x[\"KeySchema\"][0][\"AttributeName\"])\n",
    "\n",
    "\n",
    "        # Populate\n",
    "        print(\"Populating %s with %d items\" % (IMPORT_TABLES_ADD_PREFIX + table_name, len(items)))\n",
    "        table = resource.Table(IMPORT_TABLES_ADD_PREFIX + table_name)\n",
    "        with table.batch_writer() as batch:\n",
    "            for i in data[table_name][\"Items\"]:\n",
    "                batch.put_item(Item=i)\n",
    "        print(\"Done.\")\n",
    "\n",
    "    print(\"---- Data import complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
