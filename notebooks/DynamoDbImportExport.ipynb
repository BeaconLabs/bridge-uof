{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DynamoDB Import/Export Notebook\n",
    "\n",
    "Author: Everett\n",
    "\n",
    "Temporary solution to our need to back up (and possibly restore) dynamodb tables. Change the settings in the next cell, and then run the entire notebook.\n",
    "\n",
    "Good documentation on boto3, the python library for AWS APIs.\n",
    "* [boto3.resource](http://boto3.readthedocs.io/en/latest/guide/dynamodb.html) (which you should use whenever possible)\n",
    "* [boto3.client](http://boto3.readthedocs.io/en/latest/reference/services/dynamodb.html) (a more complete, but lower level API)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Configure this block, then run the whole notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RUN_EXPORT = True\n",
    "EXPORT_FILENAME = \"dynamodb.pickle\"\n",
    "\n",
    "RUN_IMPORT = False\n",
    "IMPORT_FILENAME = \"dynamodb.pickle\"\n",
    "IMPORT_TABLES_ADD_PREFIX = \"\"\n",
    "\n",
    "# AWS keys and DynamoDB region\n",
    "# If these are left blank, we'll try to find environment\n",
    "# variables by the same name and use those.\n",
    "AWS_ACCESS_KEY_ID = ''\n",
    "AWS_SECRET_ACCESS_KEY = ''\n",
    "AWS_REGION = ''\n",
    "\n",
    "# If this is true, we connect to a dynamodb-local instance\n",
    "# running on port 8000.\n",
    "USE_DYNAMODB_LOCAL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import copy\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global client, resource\n",
    "\n",
    "def set_local_db():\n",
    "    global client, resource\n",
    "    client = boto3.client('dynamodb', endpoint_url='http://localhost:8000')\n",
    "    resource = boto3.resource('dynamodb', endpoint_url='http://localhost:8000')\n",
    "\n",
    "def set_client(access=AWS_ACCESS_KEY_ID, secret=AWS_SECRET_ACCESS_KEY, region=AWS_REGION):\n",
    "    global client, resource\n",
    "    access = access or os.environ['AWS_ACCESS_KEY_ID']\n",
    "    secret = secret or os.environ['AWS_SECRET_ACCESS_KEY']\n",
    "    region = region or os.environ['AWS_REGION']\n",
    "    client = boto3.client(\n",
    "        'dynamodb', \n",
    "        aws_access_key_id=access,\n",
    "        aws_secret_access_key=secret,\n",
    "        region_name=region)\n",
    "    resource = boto3.resource(\n",
    "        'dynamodb', \n",
    "        aws_access_key_id=access,\n",
    "        aws_secret_access_key=secret,\n",
    "        region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scan_all_items(table_name, index_name=None):\n",
    "    items = []\n",
    "    start_key = None\n",
    "    table = resource.Table(table_name)\n",
    "    while True:\n",
    "        args = {}\n",
    "        if index_name:\n",
    "            args['IndexName'] = index_name\n",
    "        if start_key:\n",
    "            args['ExclusiveStartKey'] = start_key\n",
    "        response = table.scan(**args)\n",
    "        items.extend(response['Items'])\n",
    "        start_key = response.get('LastEvaluatedKey', None)\n",
    "        if start_key is None:\n",
    "            break\n",
    "        print(\"Paginated, %d items so far\" % len(items))\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_up_table_schema(schema, rename_prefix=None):\n",
    "    schema = copy.deepcopy(schema)\n",
    "    index_names = ['GlobalSecondaryIndexes', 'LocalSecondaryIndexes']\n",
    "    table_keys = ['AttributeDefinitions', 'KeySchema', 'ProvisionedThroughput',\n",
    "                 'TableName'] + index_names\n",
    "    index_keys = ['IndexName', 'KeySchema', 'Projection', 'ProvisionedThroughput']\n",
    "    if rename_prefix:\n",
    "        schema['TableName'] = rename_prefix + schema['TableName']\n",
    "        for name in index_names:\n",
    "            for index in schema.get(name, []):\n",
    "                index['IndexName'] = rename_prefix + index['IndexName']\n",
    "    \n",
    "    # Clean up indices\n",
    "    for name in index_names:\n",
    "        for index in schema.get(name, []):\n",
    "            keys = list(index.keys())\n",
    "            for k in keys:\n",
    "                if k not in index_keys:\n",
    "                    del index[k]\n",
    "                elif k == 'ProvisionedThroughput':\n",
    "                    del index[k]['NumberOfDecreasesToday']\n",
    "    # Clean up table\n",
    "    keys = list(schema.keys())\n",
    "    for k in keys:\n",
    "        if k not in table_keys:\n",
    "            del schema[k]\n",
    "        elif k == 'ProvisionedThroughput':\n",
    "            del schema[k]['NumberOfDecreasesToday']\n",
    "    return schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_table_data(name):\n",
    "    result = {}\n",
    "    result['Items'] = scan_all_items(name)\n",
    "    result['Table'] = client.describe_table(TableName=name)['Table']\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_table_data(table_prefix=''):\n",
    "    data = {}\n",
    "    table_names = [t.name for t in resource.tables.all() if t.name.startswith(table_prefix)]\n",
    "    for name in table_names:\n",
    "        data[name] = get_table_data(name)\n",
    "        describe_table_data(table_data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def describe_table_data(table_data):\n",
    "    print(\"%40s:%8d items %8.0f kB\" % (\n",
    "            name, len(table_data['Items']), table_data['Table']['TableSizeBytes'] / 1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 0: Establish a connection and make sure it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set_client()\n",
    "for t in resource.tables.all():\n",
    "    print(\"Found table\", t.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Export an entire database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if RUN_EXPORT:\n",
    "    data = get_all_table_data(table_prefix='')\n",
    "    with open(EXPORT_FILENAME , \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(\"Saved %d bytes to %s\" % (os.stat(EXPORT_FILENAME).st_size, EXPORT_FILENAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - import from pickled export (task 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pickled data from dynamodb.pickle\n",
      "** Importing into table demo_ursus_involved_civilians **\n",
      "Create operation sent. Waiting until table exists.\n",
      "Populating demo_ursus_involved_civilians with 10 items\n",
      "Done.\n",
      "** Importing into table demo_ursus_visits **\n",
      "Create operation sent. Waiting until table exists.\n",
      "Populating demo_ursus_visits with 0 items\n",
      "Done.\n",
      "** Importing into table demo_ursus_users **\n",
      "Create operation sent. Waiting until table exists.\n",
      "ACTIVE index: ori\n",
      "Populating demo_ursus_users with 2 items\n",
      "Done.\n",
      "** Importing into table demo_ursus_auditentries **\n",
      "Create operation sent. Waiting until table exists.\n",
      "Populating demo_ursus_auditentries with 0 items\n",
      "Done.\n",
      "** Importing into table demo_ursus_events **\n",
      "Create operation sent. Waiting until table exists.\n",
      "Populating demo_ursus_events with 5 items\n",
      "Done.\n",
      "** Importing into table demo_ursus_incidents **\n",
      "Create operation sent. Waiting until table exists.\n",
      "ACTIVE index: incident_id_str\n",
      "Populating demo_ursus_incidents with 10 items\n",
      "Done.\n",
      "** Importing into table demo_ursus_globalstates **\n",
      "Create operation sent. Waiting until table exists.\n",
      "Populating demo_ursus_globalstates with 1 items\n",
      "Done.\n",
      "** Importing into table demo_ursus_changedfields **\n",
      "Create operation sent. Waiting until table exists.\n",
      "Populating demo_ursus_changedfields with 0 items\n",
      "Done.\n",
      "** Importing into table demo_ursus_general_infos **\n",
      "Create operation sent. Waiting until table exists.\n",
      "Populating demo_ursus_general_infos with 10 items\n",
      "Done.\n",
      "** Importing into table demo_ursus_feedbacks **\n",
      "Create operation sent. Waiting until table exists.\n",
      "Populating demo_ursus_feedbacks with 0 items\n",
      "Done.\n",
      "** Importing into table demo_ursus_screeners **\n",
      "Create operation sent. Waiting until table exists.\n",
      "Populating demo_ursus_screeners with 10 items\n",
      "Done.\n",
      "** Importing into table demo_ursus_agencystatuses **\n",
      "Create operation sent. Waiting until table exists.\n",
      "Populating demo_ursus_agencystatuses with 1 items\n",
      "Done.\n",
      "** Importing into table demo_ursus_involved_officers **\n",
      "Create operation sent. Waiting until table exists.\n",
      "Populating demo_ursus_involved_officers with 10 items\n",
      "Done.\n",
      "---- Data import complete\n"
     ]
    }
   ],
   "source": [
    "if RUN_IMPORT:\n",
    "    print(\"Loading %d bytes of pickled data from\",\n",
    "          os.stat(IMPORT_FILENAME).st_size, IMPORT_FILENAME)\n",
    "    data = None\n",
    "    with open(IMPORT_FILENAME, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    print(\"Loaded data for %d tables\" % len(data))\n",
    "    for table_name, table_data in data.items:\n",
    "        describe_table_data(table_data)\n",
    "\n",
    "    print(\"Importing...\")\n",
    "    existing_table_names = set([t.name for t in resource.tables.all()])\n",
    "    for table_name, info in data.items():\n",
    "        new_name = IMPORT_TABLES_ADD_PREFIX + table_name\n",
    "        print(\"** Importing into table %s **\" % new_name)\n",
    "\n",
    "        # Delete table if it already exists\n",
    "        if new_name in existing_table_names:\n",
    "            print(new_name, \"already exists, deleting it first and starting afresh.\")\n",
    "            client.delete_table(TableName=new_name)\n",
    "            print(\"Delete operation sent. Waiting until table no longer exists.\")\n",
    "            resource.Table(new_name).wait_until_not_exists()\n",
    "\n",
    "        # Create a new table\n",
    "        schema = clean_up_table_schema(info[\"Table\"], rename_prefix=IMPORT_TABLES_ADD_PREFIX)\n",
    "        items = info[\"Items\"]\n",
    "        client.create_table(**schema)\n",
    "        print(\"Create operation sent. Waiting until table exists.\")\n",
    "        tbl = resource.Table(new_name)\n",
    "        tbl.wait_until_exists()\n",
    "        if tbl.global_secondary_indexes:\n",
    "            for x in tbl.global_secondary_indexes:\n",
    "                print(x[\"IndexStatus\"], \"index:\", x[\"KeySchema\"][0][\"AttributeName\"])\n",
    "\n",
    "\n",
    "        # Populate\n",
    "        print(\"Populating %s with %d items\" % (IMPORT_TABLES_ADD_PREFIX + table_name, len(items)))\n",
    "        table = resource.Table(IMPORT_TABLES_ADD_PREFIX + table_name)\n",
    "        with table.batch_writer() as batch:\n",
    "            for i in data[table_name][\"Items\"]:\n",
    "                batch.put_item(Item=i)\n",
    "        print(\"Done.\")\n",
    "\n",
    "    print(\"---- Data import complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
